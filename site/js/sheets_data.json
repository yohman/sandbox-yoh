{
    "work": [
        [
            "VSim",
            "Real-time exploration of 3D models",
            "",
            "<p>VSim facilitates the real-time exploration of highly detailed, three-dimensional computer models in both formal and informal educational settings. Funded by a Digital Humanities Implementation Grant from the National Endowment for the Humanities, this software addresses the greatest challenge for building knowledge through the use of three-dimensional computer models by providing scholars and educators the mechanism to explore, annotate, craft narratives, and build arguments within the 3D space \u2013 in essence, facilitating the creation of virtual learning environments that can be broadly disseminated to educators and learners across grade levels and humanities disciplines.</p>\n\n<h2>VSim 2.0 BETA Version Now Available!</h2>\n\n<p>VSim 2.0 BETA is now available for downloading and testing in both Windows and Mac versions! VSim 2.0 offers significant improvements over the prototype, and includes advanced font and text controls, scroll bars for both the Narrative and Embedded Resources Bars, more file import options, and improved filtering and editing capabilities for the embedded resources, among many new and improved features. The software is also responsive, so narrative overlays will adapt to changes in the size of the simulation window.  VSim was funded by the NEH, and the beta files were posted in December 2018. Please report any reproducible problems to Lisa M. Snyder.</p>\n<ul>\n<li><a href=\"../downloads/VSim_BETA_Windows_Dec2018.zip\">The Windows BETA version of VSim 2.0.</a></li>\n<li><a href=\"../downloads/VSim_BETA_Mac_Dec2018.tar.gz\">The Mac BETA version of VSim 2.0.</a></li>\n<li><a href=\"../downloads/VSim_AbridgedUsersGuide_Dec20_2018.pdf\">VSim 2.0 Abridged Users Guide.</a></li>\n<li><a href=\"../downloads/Pantheon_UCLA_Dec2018.vsim\">The Pantheon test model for use with VSim 2.0.</a></li>\n</ul>",
            "vsim.jpg",
            "https://idre.ucla.edu/vsim",
            "Chicago, Virtual learning environment, three-dimensional space, 3D computer graphics, spatial, history",
            "PIs for the VSim project are Institute for Digital Research and Education technologists Drs. Scott Friedman and Lisa M. Snyder. David Stephan was the lead programmer on the VSim 2.0 development. Other 2.0 team members were Francesca Albrezzi, Aaron J. Taber, and Sam Amin. Dr. Elaine Sullivan and Joy Guey provided UX consultation, and computer science graduate students Eduardo Poyart, Franklin Fang, and XinLi Cai built the VSim prototype. ",
            "National Endowment for the Humanities (HD-50958-10 and HK-50164-14)",
            "https://www.youtube.com/embed/MYYtDTucKL8?si=9VsZj_AZqdFuOJlt",
            "<p><strong>Posted March 19, 2014</strong>: <a href=\"https://www.youtube.com/watch?v=MYYtDTucKL8&amp;feature=youtu.be\">VSim_TheMovie</a> is a 12 minute video piece that illustrates the key features of the VSim prototype (similar enough to VSim 2.0 to be useful).</p>",
            "FineArts1_Oct26.jpg,FineArts2_Oct26.jpg,StreetofCairo1_Oct26.jpg,Trans_01_May10.jpg,UST_WCE_6-scaled.jpg,VSim2_DemoImage_Dec2018.jpg,WCE_Agriculture_UCLA_Nov5_2015.jpg,WCE_Bridge_UCLA_Nov5_2015.jpg,WCE_Caravels_UCLA_Nov5_2015.jpg,WCE_ConventofLaRabida1_UCLA_Nov5_2015.jpg,WCE_FestivalHall_UCLA_Nov5_2015.jpg,WCE_FineArtsBldg_Int_UCLA_Nov5_2015.jpg,WCE_FineArtsBldg_UCLA_Nov5_2015.jpg,WCE_Horticulture_UCLA_Nov5_2015.jpg,WCE_Illinois_UCLA_Nov5_2015.jpg,WCE_LouisianaStateBldg_UCLA_Nov5_2015.jpg,WCE_MaineStateBldg_UCLA_Nov5_2015.jpg,WCE_Manufactures_UCLA_Nov5_2015.jpg,WCE_MassStateBldg_UCLA_Nov5_2015.jpg,WCE_RoseGarden_UCLA_Nov5_2015.jpg,WCE_WisconsinStateBldg_UCLA_Nov19_2015.jpg,WomansTerrace1_Oct26.jpg",
            "Lisa Snyder",
            "Chicago, Virtual learning environment, three-dimensional space, 3D computer graphics, spatial, history",
            "http://www.wikidata.org/entity/Q1297; http://www.wikidata.org/entity/Q2349394; http://www.wikidata.org/entity/Q34929; http://www.wikidata.org/entity/Q189177; http://www.wikidata.org/entity/Q122075505; http://www.wikidata.org/entity/Q309",
            "TRUE"
        ],
        [
            "\u2018A Kind of Alchemy': The Work of Art in the Age of Artificial Intelligence (2023)",
            "International Journal of Digital Art History Gallery",
            "",
            "When Marcel Duchamp first started displaying his readymades in the early twentieth century, he sparked a conversation around the distinguishing criteria for something to be considered a work of art. Notions of authorship, process, politics, taste, and creativity were foregrounded in a way that is emerging again today as AI Art becomes more prevalent in contemporary art scenes. 'A Kind of Alchemy': The Work of Art in the Age of Artificial Intelligence will consider the medium of AI Art and its developing tools, such as DALL-E 2, Midjourney, and Stable Diffusion, along with the artists who are at the heart of this experimentation and emerging practice. ",
            "AI Art Exhibition.gif",
            "https://dahj.org/cfp/call-for-ai-artists",
            "artificial intelligence, media art, digital art, exhibition, three-dimensional space",
            "",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi",
            "artificial intelligence, media art, digital art, exhibition, three-dimensional space",
            "http://www.wikidata.org/entity/Q11660; http://www.wikidata.org/entity/Q118744998; http://www.wikidata.org/entity/Q860372; http://www.wikidata.org/entity/Q464980; http://www.wikidata.org/entity/Q34929",
            "TRUE"
        ],
        [
            "XaViz",
            "Visualizing Villages, families, and egos in the Edo Period",
            "",
            "<p>What did a household look like for common people in early modern Japan? Were there multiple generations living under one roof? Who assumes the role of household head, how often did people from outside their village come to live with them, and how many servants did each household have?</p>\n\n<p>These questions, and many more, are revealed through XaViz, a visualization of a rare digital collection of Shumon-aratame-cho (SAC) and Ninbetsu-aratame-cho (NAC), which are the two major sources for the research of historical demography in Japan. XaViz is an interactive visualization tool for longitudinal and comparative inspection, applied to the records of thousands of lives of people. Its intuitive interface will allow us to gain new insights on our history and the resilience of people to socioeconomic and environmental changes.</p>",
            "XaViz.jpg",
            "https://yohman.github.io/pfhp/web/site/about/about/",
            "population, demography, Japan, history, family, data visualization, big data, spatial analysis",
            "Population and Family History Project (PFHP) is dedicated to archive, construct and analyze population records from pre-census Japan. Building on the long endeavor of Prof. Akira Hayami and his group to establish the collection of Shumon-aratame-cho(SAC) and Ninbetsu-aratame-cho (NAC), which are the two major sources for the research of historical demography in Japan, our aim is to construct life course data records and analyze them to reveal lives and households of common people in early modern Japan. The longitudinal and comparative approach applied to the records of thousands of lives of people will allow us to gain new understanding of our history and the resilience of people to socioeconomic and environmental changes.",
            "",
            "",
            "",
            "",
            "Yoh Kawano",
            "population, demography, Japan, history, family, data visualization, big data, spatial analysis",
            "http://www.wikidata.org/entity/Q2625603; http://www.wikidata.org/entity/Q37732; http://www.wikidata.org/entity/Q17; http://www.wikidata.org/entity/Q309; http://www.wikidata.org/entity/Q8436; http://www.wikidata.org/entity/Q6504956; http://www.wikidata.org/entity/Q858810; http://www.wikidata.org/entity/Q1938983"
        ],
        [
            "Annotated Virtual Exhibitions",
            "Situational context for cultural objects",
            "",
            "Extended reality (XR) technologies encourage spatial understandings by offering situational context for cultural objects while providing access to supplemental resources such as images, audio, and text. Photogrammetric modeling of exhibitions can allow audiences to continue to learn from them long after they have been taken down. In my work, I partner with curators, exhibition designers and staff, and cultural institutions to document exhibitions and cultural spaces, building 3D models using photogrammetry. These models can be annotated with images, videos, audio, and text to create a rich immersive resource.  ",
            "photogrammetry.jpg",
            "https://fowler.ucla.edu/virtual-exhibitions/",
            "extended reality, photogrammetry, exhibition, 3D modeling, three-dimensional space, media art, online exhibition",
            "",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi",
            "extended reality, photogrammetry, exhibition, 3D modeling, three-dimensional space, media art, online exhibition",
            "http://www.wikidata.org/entity/Q25052165; http://www.wikidata.org/entity/Q190149; http://www.wikidata.org/entity/Q464980; http://www.wikidata.org/entity/Q568742; http://www.wikidata.org/entity/Q34929; http://www.wikidata.org/entity/Q118744998; http://www.wikidata.org/entity/Q3062261"
        ],
        [
            "Vital Matters",
            "Digital initiative representing different perspectives at the Fowler",
            "",
            "\u201cVital Matters / Art, Devotion, Practice\u201d is a digital initiative that represents different perspectives on devotional works at the Fowler Museum\u2014objects that arouse devotion, awe, and serenity; mediate relationships between human and spiritual realms; and are of vital importance to the cultural heritage of individuals and communities.",
            "vitalmatters.png",
            "https://fowler.ucla.edu/vital-matters/",
            "art, academic, UCLA Fowler Museum of Cultural History",
            "",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi",
            "art, academic, UCLA Fowler Museum of Cultural History",
            "http://www.wikidata.org/entity/Q735; http://www.wikidata.org/entity/Q3400985; http://www.wikidata.org/entity/Q7864026"
        ],
        [
            "Visual and Sonic Landscapes of Muslims in Los Angeles",
            "A focus on community voices and ritual, music, murals, and other artworks as primary source materials",
            "",
            "The \u201cVisual and Sonic Landscapes in the City of Angels\u201d initiative is created to share the lived experiences of faith-based communities and cultural expressions of spirituality across Los Angeles through sights and sounds. We believe that these manifestations embody individual and group values, ideologies, and worldviews. Often central components of lived religion, visual and aural practices are vivid and tangible forms of belief. We envision this website as an ever-expanding resource for students, faculty, and K-12 classrooms, helping them engage with the histories and current realities of spiritually-based communities in Los Angeles. ",
            "visualsonic.png",
            "https://fowler.ucla.edu/visual-sonic-landscapes/",
            "UCLA Fowler Museum of Cultural History, art, primary source, sound, visual perception, media art",
            "Asma Sayeed (Co PI), Edina Lekovic (Community Scholar in Residence), Amy Landau (Co PI), Vikas Malhotra (Researcher), Rhana Tabrizi (Curatorial Assistant), Abeer-Ramadan-Shinnawi (K-12 Educator), Marina Belozerskaya (Editor), Leigh Carter (Project Manager and Educator), Ruth Keffer (Editor), Francesca Albrezzi (Director of Digital Development for Vital Matters), Ryan Horne (Web Developer), Johnathan Glover (Head of Digital Learning and Innovation), Dekker Dreyer (Video Editor)",
            "Visual and Sonic Landscapes of Muslim LA was made possible through the generous support of The Henry Luce Foundation, Lilly Endowment Inc., and the UCLA Chancellor's Art Initiative Grant. This is a collaborative project between The Fowler Museum at UCLA and UCLA's Islamic Studies Program.",
            "",
            "",
            "",
            "Francesca Albrezzi, Ryan Horne",
            "UCLA Fowler Museum of Cultural History, art, primary source, sound, visual perception, media art",
            "http://www.wikidata.org/entity/Q106989501; http://www.wikidata.org/entity/Q735; http://www.wikidata.org/entity/Q112754; http://www.wikidata.org/entity/Q11461; http://www.wikidata.org/entity/Q162668; http://www.wikidata.org/entity/Q118744998"
        ],
        [
            "Digital Tendons",
            "Enhancing students\u2019 learning about the history of knowledge of human anatomy through 3D representation.",
            "",
            "The GIS and Visualization group are helping to bring the history of science to life. In partnership with Professor Chien-Ling Liu Zeleny, Professor Anthony Friscia, and three UCLA Library departments, the group (working under the umbrella of the Digital Research Consortium) has 3D scanned and 360 captured specimens in the UCLA anatomy lab. The interdisciplinary team is building a 360 tour to bring the anatomy lab experience, led by Professor Anthony Friscia, to the medical history classroom, in a course taught by Professor Liu Zeleny. ",
            "vesalius.jpg",
            "",
            "extended reality, photogrammetry, exhibition, 3D modeling, three-dimensional space, science, art, anatomy, history, 360 capture, potree",
            "Chien-Ling Liu Zeleny, PhD. Instructor of history of medicine related courses. Continuing Lecturer, UCLA History Department and Institute for Society and Genetics.\nChristopher Gilman, PhD. Digital Curriculum Program Coordinator, Digital Library Program, UCLA Library.\nDoug Daniels, MLIS. Emerging Technologies Librarian, UCLA Library.\nAnthony Friscia, PhD. Director of the Anatomy Lab. Associate Adjunct Professor, UCLA Integrative Biology & Physiology. Director of the UCLA Cluster Program. \nFrancesca Albrezzi, PhD. Manager and Digital Research Specialist, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program. ",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi",
            "extended reality, photogrammetry, exhibition, 3D modeling, three-dimensional space, science, art, anatomy, history",
            "http://www.wikidata.org/entity/Q25052165; http://www.wikidata.org/entity/Q190149; http://www.wikidata.org/entity/Q464980; http://www.wikidata.org/entity/Q568742; http://www.wikidata.org/entity/Q34929; http://www.wikidata.org/entity/Q336; http://www.wikidata.org/entity/Q735; http://www.wikidata.org/entity/Q514; http://www.wikidata.org/entity/Q309"
        ],
        [
            "Classical Vietnamese Digital Encyclopedia",
            "Digital initative for display, analysis, and data interoperability",
            "",
            "In support of Professor George Dutton (Department of Asian Languages and Cultures), the GIS and Visualization group is preparing a digital platform which will enable the transcription, annotation, and sharing of classical Vietnamese texts, offering a side-by side view of the oroginals and computer-aided transcriptions. This system will also build a digital gazetteer of place names and individuals mentioned in texts; this information, shared on partnership with other language initatives at Columbia, is forming the core of a new digital ecosystem for the study of Vietnamese language, history, and culture.",
            "dutton.png",
            "`",
            "text analysis, history, linguistics, geographic information system, gazetteer, linked open data",
            "George Dutton, PhD.,  Department of Asian Languages and Culture, project lead. Ryan Horne, PhD., Digital Research Specialist, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program. Francesca Albrezzi, PhD. Digital Research Consultant, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program.  ",
            "",
            "",
            "",
            "",
            "Ryan Horne, Francesca Albrezzi",
            "text analysis, history, linguistics, geographic information system, gazetteer, linked open data",
            "http://www.wikidata.org/entity/Q67224805; http://www.wikidata.org/entity/Q309; http://www.wikidata.org/entity/Q8162; http://www.wikidata.org/entity/Q483130; http://www.wikidata.org/entity/Q1006160; http://www.wikidata.org/entity/Q18692990"
        ],
        [
            "Hidden Histories of Ghana\u2019s Slave Forts and Castles, 1482-2022",
            "Exploring Ghana\u2019s role in the transatlantic slave trade and the religious and spiritual traditions that resulted from it.",
            "",
            "The GIS and Visualization RTG is contributing to a project led by UCLA Professor Andrew Apter that explores Ghana\u2019s role in the transatlantic slave trade and the religious and spiritual traditions that resulted from it. They assisted the project team in establishing a metadata schema for the digital archive, prototyping an interactive mapping interface, digitizing field research videos, and setting up a public-facing digital archive of related materials. The project will result in a digital presentation of Ghanaian rituals, fetishes, and deities, as well as in the mapping and digitization of the remnants and influence of European settler colonialism in Ghana",
            "slaveforts-storymap.png",
            "",
            "map making, digital archive",
            "Andrew Apter, PhD., Department of History, Project Lead. Francesca Albrezzi, PhD., Manager and Digital Research Specialist, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program. ",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi",
            "map making, digital archive",
            "http://www.wikidata.org/entity/Q124605947; http://www.wikidata.org/entity/Q1224984"
        ],
        [
            "SAKE: A Structural Analysis Knowledge Engine",
            "Creating an online statistical helper and learning tool",
            "",
            "Supporting the work of Henry Burton, PhD., School of Engineering, currently SAKE is a web-based platform for structural engineering education that uses Python code to generate solutions to fundamental structural analysis problems.  The code reads input from csv files that specify a truss, and it then uses three structural analysis methods (Method of Joints, Principle of Virtual Forces Method, and Stiffness Method) to determine unknown quantities such as unknown forces at truss nodes.  The code outputs a figure of the input-specified truss and a detailed description of the steps that are followed to determine the problem solution. In addition, the platform can grow to accomidate different structural engineering problems and solutions.",
            "",
            "",
            "statistics, Python, World Wide Web",
            "Henry Burton, PhD., School of Engineering, Project lead. Ben Winjum, Computational Scientist, Computational Science, OARC, lead developer. Ryan Horne, PhD., Digital Research Specialist, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program. ",
            "",
            "",
            "",
            "",
            "Ryan Horne",
            "statistics, Python, World Wide Web",
            "http://www.wikidata.org/entity/Q12483; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q466"
        ],
        [
            "Instructional Design Lab",
            "A Tinker Space for Innovative Curriculum Development Support",
            "",
            "The Digital Library Program (DLP) hosts drop-in and semi-programmed instructional design support workshops for faculty, lecturers, and staff interested in integrating Library Digital Collections materials, along with digital tools and platforms for inquiry-based learning and research in their courses.\n\nThe purpose of the Instructional Design Lab is to connect interested faculty with the technologies, expertise, and support services for innovative curriculum. The aim is to form a community of support - If we can\u2019t help you, we know someone who can. OARC sandbox team members are regular participants and collaborate with the DLP to produce workshops on digital tools for research and pedagogy. \n\nTopics for the Instructional Design group include:\n\n- Bruin Learn multimodal lab, assignment, and module design\n- Collections-based inquiry pedagogies\n- IIIF content and tools\n- Digital projects (e.g. Scalar, StoryMaps, etc)\n- 3D, XR, and 360 video\n\nPlease feel free to contact Chris Gilman, Digital Curriculum Program Coordinator for the DLP, with any questions at cjgilman@library.ucla.edu.",
            "LMS-Design.jpeg",
            "",
            "learning management system, digital learning, International Image Interoperability Framework, canvas, bruinlearn, pedagogy, collections-based learning",
            "Christopher Gilman, PhD, Digital Curriculum Program Coordinator and Lab Lead ",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi, Ryan Horne",
            "learning management system, pedagogy, digital learning, International Image Interoperability Framework",
            "http://www.wikidata.org/entity/Q694007; http://www.wikidata.org/entity/Q7922; http://www.wikidata.org/entity/Q16628824; http://www.wikidata.org/entity/Q22682088"
        ],
        [
            "XRI",
            "Extended Reality (XR) Initiative",
            "",
            "UCLA XR Initiative (XRI) serves as a hub for UCLA faculty and students to meet, discuss, and practically experiment with VR/AR/MR and related technologies. XRI provides a space to explore imaginative ways of teaching, learning, and creating knowledge with and along XR. UCLA XRI aims to both catalyze and facilitate transdisciplinary conversations and collaborations surrounding XR to generate innovative research, artistic practices, and discovery of creative solutions to our world\u2019s grand challenges.",
            "xri-bear-logo-1030x646.png",
            "https://xri.ucla.edu/",
            "extended reality, virtual reality, augmented reality, mixed reality, photogrammetry, lidar, 3D modeling, three-dimensional space, 360 capture",
            "Joy Guey, Emerging Technologies Advocate / Developer, Social Science Computing. Francesca Albrezzi, PhD., Manager and Digital Research Specialist, Office of Advanced Research Computing Lecturer, UCLA Digital Humanities Program. ",
            "",
            "",
            "",
            "",
            "Francesca Albrezzi, Lisa Snyder",
            "extended reality, virtual reality, augmented reality, mixed reality, photogrammetry, lidar, 3D modeling, three-dimensional space",
            "http://www.wikidata.org/entity/Q25052165; http://www.wikidata.org/entity/Q170519; http://www.wikidata.org/entity/Q254183; http://www.wikidata.org/entity/Q1758389; http://www.wikidata.org/entity/Q190149; http://www.wikidata.org/entity/Q504027; http://www.wikidata.org/entity/Q568742; http://www.wikidata.org/entity/Q34929"
        ]
    ],
    "learn": [
        [
            "Teaching and Research with Digital Collections - Part 1 (2023)",
            "Francesca Albrezzi",
            "video workshop",
            "<p>Within the scholarship of teaching and learning, it has been proven time and again that \u201cAssembling and curating specimen collections is a valuable educational exercise that integrates subject-specific skills such as field collection, curation, identification, organization, and interpretation of relationships.\u201d (Lucky et. al 2019) Digital collections increasingly avail students and scholars of primary source research materials that were only accessible previously by exclusive arrangement in person, if at all. Rare and unique items, such as manuscript documents, cultural artifacts, and historical print ephemera, still present methodological challenges for intensive engagement, even when they are readily and openly accessed. In this third workshop of the series, the focus will shift to strategies for the design, development, and implementation of curriculum-based learning activities that engage collections of IIIF (International Image Interoperability Framework) content.</p>\n<p>\nIn this workshop, participants will:\n<ul>\n<li>Learn various strategies for learning activity design using curated collections of items</li>\n<li>Create usable sample learning activities in Bruin Learn, using IIIF-based collections content and embedded IIIF-compatible tools</li>\n</ul>\n<p>\nAll levels are welcome. This workshop series is a collaboration between the UCLA Digital Library Program and the Office of Advanced Research Computing. Additional leadership provided by UCLA faculty who have been participating in the LMS Design Lab. \n</p>\n<h3>References</h3>\n<p>\nLucky, A., Branham, M. & Atchison, R. Collection-Based Education by Distance and Face to Face: Learning Outcomes and Academic Dishonesty. J Sci Educ Technol 28, 414\u2013428 (2019). https://doi.org/10.1007/s10956-019-97...",
            "digital collections1.jpg",
            "https://youtu.be/SuPGRmryCqI",
            "International Image Interoperability Framework, digital library, digital image",
            "<iframe width=\"562\" height=\"382\" src=\"https://www.youtube.com/embed/SuPGRmryCqI\" title=\"Teaching and Research w/ Digital Collections (Part 1)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "International Image Interoperability Framework, digital library, digital image",
            "http://www.wikidata.org/entity/Q22682088; http://www.wikidata.org/entity/Q212805; http://www.wikidata.org/entity/Q1250322",
            "TRUE"
        ],
        [
            "Teaching and Research with Digital Collections - Part 2 (2023)",
            "Francesca Albrezzi",
            "video workshop",
            "<p>The active engagement of digital primary source collections content can, and often does, go beyond viewing materials on collections websites. For sustained inquiry, a wide variety of tools and platforms have been developed that support in-depth scholarly analysis of high quality, reliably-hosted image-based digital content and enable the creation of public-facing scholarly work products. This workshop is the second in a series about teaching and research with digital collections, which highlights the affordances of IIIF content (for International Image Interoperability Framework) but it does not require prior knowledge or skills from the first workshop. It will focus upon the \u201cinteroperable\u201d use of digital materials outside of their original institutional context, in both lightweight, easy-to-use tools for analysis, presentation and annotation (such as Storiiies, Exhibit.so, Mirador, and MISE), and more robust platforms for digital scholarship, such as Scalar. \n<p>\nIn this workshop, participants will learn: \n<ul>\n<li> What are IIIF manifests and image URLs, and how can they be used in IIIF-compatible tools and platforms?\n<li> How to extract details from image-based content\n<li> How to import materials as linked content into various platforms\n<li> How to annotate, caption, and present content in multimedia essays and exhibits\n</ul>\n<p>\nAll levels are welcome. This workshop series is a collaboration between the UCLA Digital Library Program and the Office of Advanced Research Computing. Additional leadership provided by UCLA faculty who have been participating in the LMS Design Lab.\n</p>",
            "digital collections2.jpg",
            "https://youtu.be/_xOnkUfSpM8?si=7nBNgLs-CkoELnlB",
            "International Image Interoperability Framework, digital library, digital image",
            "<iframe width=\"562\" height=\"382\" src=\"https://www.youtube.com/embed/_xOnkUfSpM8\" title=\"Teaching &amp; Research w/ Digital Collections (Part 2)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "International Image Interoperability Framework, digital library, digital image",
            "http://www.wikidata.org/entity/Q22682088; http://www.wikidata.org/entity/Q212805; http://www.wikidata.org/entity/Q1250322",
            "TRUE"
        ],
        [
            "Teaching and Research with Digital Collections - Part 3 (2023)",
            "Francesca Albrezzi",
            "video workshop",
            "<p>Within the scholarship of teaching and learning, it has been proven time and again that \u201cAssembling and curating specimen collections is a valuable educational exercise that integrates subject-specific skills such as field collection, curation, identification, organization, and interpretation of relationships.\u201d (Lucky et. al 2019) Digital collections increasingly avail students and scholars of primary source research materials that were only accessible previously by exclusive arrangement in person, if at all. Rare and unique items, such as manuscript documents, cultural artifacts, and historical print ephemera, still present methodological challenges for intensive engagement, even when they are readily and openly accessed. In this third workshop of the series, the focus will shift to strategies for the design, development, and implementation of curriculum-based learning activities that engage collections of IIIF (International Image Interoperability Framework) content.\n<p>In this workshop, participants will:\n<ul>\n<li> Learn various strategies for learning activity design using curated collections of items\n<li> Create usable sample learning activities in Bruin Learn, using IIIF-based collections content and embedded IIIF-compatible tools\n</ul>\n<p> All levels are welcome. This workshop series is a collaboration between the UCLA Digital Library Program and the Office of Advanced Research Computing. Additional leadership provided by UCLA faculty who have been participating in the LMS Design Lab. \n\n<p>References: Lucky, A., Branham, M. & Atchison, R. Collection-Based Education by Distance and Face to Face: Learning Outcomes and Academic Dishonesty. J Sci Educ Technol 28, 414\u2013428 (2019). https://doi.org/10.1007/s10956-019-97..",
            "digital collections3.jpg",
            "https://youtu.be/N1ZuugRm04E",
            "International Image Interoperability Framework, digital library, digital image",
            "<iframe width=\"562\" height=\"382\" src=\"https://www.youtube.com/embed/N1ZuugRm04E\" title=\"Teaching &amp; Research w/ Digital Collections (Part3): Lessons and Learning Activity Design\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "International Image Interoperability Framework, digital library, digital image",
            "http://www.wikidata.org/entity/Q22682088; http://www.wikidata.org/entity/Q212805; http://www.wikidata.org/entity/Q1250322"
        ],
        [
            "Introduction to Network Analysis Methodologies and Tools",
            "Ryan Horne",
            "video workshop",
            "Are you interested in the connections between people? Places? Concepts? Things? Do you want to visualize data in a way that is comprehensible to a human? Do you want to see how information flows through systems? Then network analysis is for you! This introductory workshop will introduce participants to network analysis, with an emphasis on its use for data-driven humanities research. Upon completion of the workshop, participants will be familiar with important concepts and trends in network analysis, be able to construct a basic network from a spreadsheet, run common network analytics, and visualize the results. No prior knowledge of network analysis or programming is required or expected.",
            "network analysis.jpg",
            "https://youtu.be/V_liCwE_ZoI?si=LDrNVYBwQozz1ODI",
            "network analysis, Python, Gephi",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V_liCwE_ZoI?si=LDrNVYBwQozz1ODI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "network analysis, Python, Gephi",
            "http://www.wikidata.org/entity/Q4417999; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q5548660"
        ],
        [
            "Digital Humanities and HPC",
            "Ryan Horne",
            "video workshop",
            "In this presentation, we will explore how the HPC can be used to accomplish common DH tasks, such as network and GIS analysis with large data sets which would be computationally difficult or expensive on typical desktop / laptop hardware. We will use least-cost pathing with GDAL, network analysis with Python, and general DH data analysis and visualization with Jupyter Notebooks. This workshop is designed to highlight the usability and accessibility of these common tools in the HPC environment.",
            "dh and hpc.jpg",
            "https://youtu.be/08MqApQa-9w?si=2UVLTDJ-4wltX0y5",
            "digital humanities, high-performance computing, geographic information system, Python, Jupyter Notebook",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/08MqApQa-9w?si=2UVLTDJ-4wltX0y5\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "digital humanities, high-performance computing, geographic information system, Python, Jupyter Notebook",
            "http://www.wikidata.org/entity/Q1026962; http://www.wikidata.org/entity/Q1190465; http://www.wikidata.org/entity/Q483130; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q105099901"
        ],
        [
            "Getting Started With GIS and Spatial Research: Census data analysis with Python",
            "Yoh Kawano",
            "video workshop",
            "In this hands on workshop, you will be provided with a Jupyter Notebook workspace to import census data, explore, clean, and prepare the data for analysis using python. You will also learn how to create compelling choropleth maps with selected variables that can be used for publications.",
            "getting started gis.jpg",
            "https://youtu.be/QLJDp1XRjWA?si=bWC_Xul92Y4ZhTOE",
            "geographic information system, Python, United States census, spatial analysis, data science",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QLJDp1XRjWA?si=bWC_Xul92Y4ZhTOE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>",
            "geographic information system, Python, United States census, spatial analysis, data science",
            "http://www.wikidata.org/entity/Q483130; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q1345528; http://www.wikidata.org/entity/Q1938983; http://www.wikidata.org/entity/Q2374463"
        ],
        [
            "Annotate Digital Imagery: Theory, Applied Practice, and Tools for Research (2024)",
            "Francesca Albrezzi",
            "video workshop",
            "Information systems since the advent of the World Wide Web have offered increasingly sophisticated and reliable tools for inquiry and scholarship based upon centuries-old practices of annotation. What are the connections and tensions between visual and verbal media? How do idiosyncratic readerly practices of scribbling in margins of a book translate into W3C standards for networked multimedia scholarly communication, and what impacts will community standards development have upon the future of knowledge production (or your career)? This workshop will provide a theoretical introduction to the ancient and still evolving social practice of annotation and situate leading ideas in the concrete tools and practices of digital knowledge production. Taking a 3-tiered approach (simple, intermediate, complex), it will present several handy tools for immediate and long-term application. This workshop is part of the Research Collections and Digital Scholarship series \u2014 a collaboration between the Office of Advanced Research Computing (OARC) and the UCLA Digital Library Program.",
            "annotate digital images.png",
            "https://www.youtube.com/watch?v=igOv6kYhw5I",
            "digital humanities, annotation, story, Scalar (publishing platform), Tropy, Mirador",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/igOv6kYhw5I?si=PpxPFG30MZRDOYb1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "digital humanities; annotation; story; Scalar (publishing platform); Tropy",
            "http://www.wikidata.org/entity/Q1026962; http://www.wikidata.org/entity/Q857525; http://www.wikidata.org/entity/Q217086; http://www.wikidata.org/entity/Q75941967; http://www.wikidata.org/entity/Q56278333"
        ],
        [
            "Intro to Immersive Annotated 360 Virtual Tours with Adobe Captivate (2024)",
            "Francesca Albrezzi",
            "video workshop",
            "Most would agree that in terms of creating virtual experiences as part of research, more context is better. Contextual information allows for a bigger picture to be seen and can provide in situ understanding, despite being remote. With the proliferation of 360 cameras, creating immersive virtual tours has become more commonplace. Whether it is to document the layout of a hydroelectric power plant, provide archival information about historical relics, such as the HMAS OVENS submarine, or share information regarding a cultural heritage site, such as Google Arts and Culture\u2019s Street View tours, immersive 360 capture with annotations has become a strong visual research method and product. In addition, many fields of research look to present information or scenarios and then survey a sample of participants regarding their experience. Interactive surveys or modules can provide valuable research data for any number of disciplines. In this workshop, participants will be introduced to Adobe Captivate, a software for building e-learning interactives and annotated 360 virtual tours. Focusing on examples that bridge UCLA\u2019s Biomedical Library space and digital collections, participants will learn how to build a virtual tour with 360 images and video. They will also learn how to annotate their tours with collections materials and how to build some of the platforms various interactive activities, which include: Multiple choice True/False Fill-in-the-blank Short Answer Matching Hot Spot Sequence Rating scale Random question No prior experience with Adobe products will be needed. However, this product is not part of the Adobe Creative Cloud Suite. If participants want to follow along during the workshop, they will need to purchase and download Adobe Captivate before the workshop. This workshop is part of the Research Collections and Digital Scholarship series \u2014 a collaboration between the Office of Advanced Research Computing (OARC) and the UCLA Digital Library Program.",
            "intro to 3d virtual tours.png",
            "https://www.youtube.com/watch?v=uf5LBhTohac",
            "Adobe Captivate, virtual tour, 360",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/uf5LBhTohac?si=yFL0PQ3YkS1wSfgD\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "Adobe Captivate, virtual tour",
            "http://www.wikidata.org/entity/Q360071; http://www.wikidata.org/entity/Q2915546"
        ],
        [
            "Getting Started with Digital Film in Research: Managing, Editing and Best Practices",
            "Francesca Albrezzi",
            "video workshop",
            "Whether it is interviews, documentary coverage, or contextual footage, video has become a common practice within research. This course is designed to help scholars manage and make use of their research video footage. The workshop will discuss various best practices regarding file management, editing platforms, and techniques for putting footage together to meet your research goals. Participants will be introduced to Davinci Resolve to learn the basics of a preliminary editing process with industry standard software. Software alternatives, such as Adobe Premiere and iMovie will be discussed. In addition, related resources will be shared so that participants can continue to progress in the video editing process. Any questions about this workshop can be emailed to Francesca Albrezzi at falbrezzi@ucla.edu. Slides: https://docs.google.com/presentation/...",
            "digital film.png",
            "https://www.youtube.com/watch?v=xZJuQPOjzZc",
            "Adobe Premiere Pro, digital cinema, research, DaVinci Resolve, video editing, iMovie",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xZJuQPOjzZc?si=g6k2gwEk2G822KWk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "Adobe Premiere Pro; digital cinema; research; DaVinci Resolve; video editing; iMovie",
            "http://www.wikidata.org/entity/Q360424; http://www.wikidata.org/entity/Q761706; http://www.wikidata.org/entity/Q42240; http://www.wikidata.org/entity/Q17628502; http://www.wikidata.org/entity/Q1154312; http://www.wikidata.org/entity/Q9558"
        ],
        [
            "Leveling Up Your Zoom with OBS Studio",
            "Francesca Albrezzi",
            "video workshop",
            "<p>Have you ever seen someone zooming or live streaming with a great border or graphic overlay and wondered how they did it? Learn how to do it yourself in this workshop! Zoom backgrounds are nice, but borders and overlays can take your casting to the next level and enhance your professional brand. In this workshop, you will learn how to use OBS Studio, a free, open-source, and cross-platform screencasting and streaming app, to add a wow-factor that will set your zoom apart.\n<p>\nWe will cover:\n<ul>\n<li> How to install and set up OBS Studio\n<li> How to connect and stream with OBS Studio to your Zoom set up \n<li> How to create custom overlays with PowerPoint that have transparency, 3D elements, and animated effects\n</ul>\n</p>",
            "obs zoom.png",
            "https://www.youtube.com/watch?v=XXY_66UjJMI",
            "Microsoft PowerPoint, OBS Studio, Zoom, overlays",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XXY_66UjJMI?si=He3MXXvmLp2ld4h7\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "Microsoft PowerPoint, OBS Studio; Zoom",
            "http://www.wikidata.org/entity/Q11266; http://www.wikidata.org/entity/Q21707789; http://www.wikidata.org/entity/Q94979732"
        ],
        [
            "How to Zoom Like a Pro",
            "Francesca Albrezzi",
            "video workshop",
            "<p>Ever wonder how some people look like newcasters when they are Zooming? In this hour-long workshop, we will cover Zoom settings, external equipment options, and lightning and camera techniques that will help you take your Zoom presentation to the next level. \n<p>\nBy the end of the workshop, you will learn: \n<ul>\n<li> How to adjust your video and audio settings in Zoom for the best performance\n<li> How to set up the use of an external camera and microphone and what types of equipment will enhance your picture and audio quality\n<li> What kind of lighting will help you shine and what kind of lighting tools you will want to invest in\n<li> Things to avoid that will distract from your Zoom appearance\n</ul>\n</p>\n<p>This session will be led by Digital Research Consultant Francesca Albrezzi, PhD and Senior Program Manager Eleanor Koehl from the Office of Advanced Research Computing (OARC) at UCLA.",
            "zoom like a pro.png",
            "https://www.youtube.com/watch?v=cBTRTQuRJjk",
            "zoom, video equipment, lighting technique, camera techniques",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cBTRTQuRJjk?si=8S2Y3on035C8DCSF\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "Zoom, lighting technique",
            "http://www.wikidata.org/entity/Q94979732; http://www.wikidata.org/entity/Q17177506"
        ],
        [
            "NFTs: Getting Started in New Digital Markets",
            "Francesca Albrezzi",
            "video workshop",
            "NFTs seem to be everywhere now. NFT stands for non fungible tokens. The creation and exchange of NFTs has been revolutionary in many ways, particularly in terms of arts and services markets. For example, in the art market, NFTs use blockchain technology to allow for the secure transfer and proof that an original piece of digital art has been sold and is now in the possession of the buyer. It also allows for what we in art history may refer to as provenance of an artwork to be more carefully tracked. The benefits of blockchain is that it enables a global secure transfer of private smart contracts between parties that are stored on an open decentralized network. The result has exposed art market sales more openly and shifted the value of digital art drastically. In this workshop you will learn about rarity, royalties, and crypto wallets. We will look at some examples of the way NFTs are being used in different markets, but will focus most specifically on growing digital art markets. You will also learn how to set up your own crypto wallet, create your own NFT, and begin mining Ethereum, a popular crypto currency, which is used in the buying and selling of many NFT artworks and items. No prior knowledge or experience required. Any questions about this workshop can be emailed to falbrezzi@oarc.ucla.edu.",
            "nft.png",
            "https://www.youtube.com/watch?v=cdVPLtm-85k",
            "non-fungible token, cryptocurrency, cryptocurrency wallet, cryptocurrency mining, digital art market",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cdVPLtm-85k?si=i2RXEJxLM4KkKht0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "non-fungible token, cryptocurrency, cryptocurrency wallet, cryptocurrency mining",
            "http://www.wikidata.org/entity/Q55648452; http://www.wikidata.org/entity/Q13479982; http://www.wikidata.org/entity/Q40186999; http://www.wikidata.org/entity/Q21769973"
        ],
        [
            "Getting Started with Comparing and Annotating Digital Images - Part 1",
            "Francesca Albrezzi",
            "video workshop",
            "Scholarship of visual materials requires close looking and comparison across works. Digitization efforts across the world have allowed for many digital facsimiles of these materials to be made available online. For scholarship, digital viewers are extremely valuable to presenting high-resolution imaging, which allows for close examination of materials. Interoperability has become crucial for institutions for contrasting objects from different collections. Increasing stability in digital visual scholarship means working from authority files from museums, libraries, and archives \u2014 institutions are committed to long term preservation and standards. This workshop will introduce participants to Mirador, an \u201copen-source, web based, multi-window image viewing platform with the ability to zoom, display, compare and annotate images from around the world.\u201d Leveraging the International Image Interoperability Framework (IIIF), the Mirador web-viewer allows users to compare and annotate images and documents. Participants will learn how to install a local instance of Mirador, find Mirador-friendly content and resources, make narratives with annotations, and complete side-by-side transcriptions. All levels are welcome.",
            "compare annotate images.png",
            "https://www.youtube.com/watch?v=cE04GhZ1pHs",
            "digital image, annotation, International Image Interoperability Framework, mirador",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cE04GhZ1pHs?si=-ieRaef0C-BpoxDy\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "digital image, annotation, International Image Interoperability Framework",
            "http://www.wikidata.org/entity/Q1250322; http://www.wikidata.org/entity/Q857525; http://www.wikidata.org/entity/Q22682088"
        ],
        [
            "Introduction to Photogrammetry and Mobile LiDAR Scanning",
            "Francesca Albrezzi",
            "video workshop",
            "3D models can be formed in a number of ways. Two common surveying methods that result in 3D models are photogrammetry and light detection and ranging or lidar for short. A form of photogrammetry has been around since the beginnings of photography in the mid-19th century. The term usually refers to the method by which 2D images are compared and combined using optics, projective geometry, and computing to produce a 3D model. This method of image overlap to create a dimensional model is applied in many object-oriented disciplines such as archeology, topographic mapping, architecture, engineering, manufacturing, quality control, police investigation, cultural heritage, and geology. Lidar is a method for measuring distances (ranging) by illuminating the target with laser light and measuring the reflection with a sensor. Differences in laser return times and wavelengths can then be used to make digital 3D representations of the target. While long used in the earth sciences, lidar is becoming increasingly available to all as these sensors are being added to cellular mobile devices, increasing its academic and commercial potential. In this workshop, participants will learn about the hardware and software required for these methods. We will also cover the workflow that is involved in producing photogrammetric and lidar 3D models. All levels welcome. If you have any further questions regarding the workshop, please contact the instructor, Dr. Francesca Albrezzi at falbrezzi@ucla.edu",
            "photogrammetry.png",
            "https://www.youtube.com/watch?v=RPFkWrEluqo",
            "lidar, photogrammetry, 3D modeling, mushroom, MeshLab, Google Poly, cavas pocket",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/RPFkWrEluqo?si=lzN0j84392qtxCsa\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "lidar, photogrammetry, 3D modeling, mushroom, MeshLab, Google Poly",
            "http://www.wikidata.org/entity/Q504027; http://www.wikidata.org/entity/Q190149; http://www.wikidata.org/entity/Q568742; http://www.wikidata.org/entity/Q83093; http://www.wikidata.org/entity/Q3306840; http://www.wikidata.org/entity/Q100573674"
        ],
        [
            "Introduction to Augmented Reality with Artivive",
            "Francesca Albrezzi",
            "video workshop",
            "Augmented reality (AR) is the layering of computer-generated perceptual information onto real environments. The experience requires a level of interactivity and sensory engagement. AR is being adopted across the fields of architecture and engineering, as well as adopted by artists to make creative connections between our shared reality and that of innumerable virtual worlds. This workshop will introduce participants to the design and mechanics of augmented reality. We will focus on working with images, bringing digital objects into live environments through the use of the ArtVive app. Attendees will create free Artivive accounts and learn to create an AR animation. No prior technical experience necessary.",
            "ar artvive.png",
            "https://www.youtube.com/watch?v=A9Z6LnQb0C8",
            "augmented reality, animation, artvive",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/A9Z6LnQb0C8?si=IQLidZjpXjZJE0aL\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "augmented reality, animation",
            "http://www.wikidata.org/entity/Q254183; http://www.wikidata.org/entity/Q11425"
        ],
        [
            "Introduction to Three Dimensional Graphing and WebVR",
            "Francesca Albrezzi",
            "video workshop",
            "<p>We are increasingly sharing our data and research in virtual environments. WebVR (or WebXR as it\u2019s now known) is an open standard that allows 3D / virtual reality (VR) experiences to be produced through your browser. It also aims to make your immersive data platform-agnostic, so it will be portable to whatever device you have. In this workshop, you will be introduced to ways to graph three dimensional data. You will also be shown how to bring that 3D graph into a browser environment so that your work can be shared through the Web.\n<p>\nIn the hands-on segment of the workshop, we will be using the following tools:\n<ul>\n<li> R\n<li> A-Frame\n<li> Jupyter Notebooks\n</ul>\n<p>There are no pre-requisite requirements to take this workshop.",
            "intro webvr.png",
            "https://www.youtube.com/watch?v=nCp94r5IciE",
            "WebXR, virtual reality, R, A-Frame, Jupyter Notebook, 3D graphing",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nCp94r5IciE?si=AW5uE8IEv4g1Pcos\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "WebXR, virtual reality, R, A-Frame, Jupyter Notebook",
            "http://www.wikidata.org/entity/Q85868115; http://www.wikidata.org/entity/Q170519; http://www.wikidata.org/entity/Q206904; http://www.wikidata.org/entity/Q28135597; http://www.wikidata.org/entity/Q105099901"
        ],
        [
            "Zoomed Out? Creating Alternative VR Environments with Mozilla Hubs",
            "Francesca Albrezzi",
            "video workshop",
            "While we are working remotely, there are many ways that we can share virtual space. Mozilla Hubs is a Web-based platform that provides users with three dimensional virtual space that can allow for links, videos, 2D and 3D objects, and live interaction. As an alternative to zoom, Mozilla Hubs has been used for poster sessions, conferences, performances, exhibitions, and more. It can also be accessed through a virtual reality headset for an immersive experience. In this workshop, participants will be introduced to Mozilla Hubs and how to use the built-in functionality. For more customized environments, we will also cover how to build your own basic environment and bring it into Mozilla Hubs through Spoke, Mozilla Hub\u2019s web editor. If you have any further questions regarding the workshop, please contact the instructor, Dr. Francesca Albrezzi at falbrezzi@ucla.edu.",
            "mozilla hubs.png",
            "https://www.youtube.com/watch?v=UWbU6zJILyY",
            "virtual reality, Mozilla Hubs, Zoom, spoke",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UWbU6zJILyY?si=1VLpOO3MXWSR_AsV\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "virtual reality, Mozilla Hubs, Zoom",
            "http://www.wikidata.org/entity/Q170519; http://www.wikidata.org/entity/Q100518757; http://www.wikidata.org/entity/Q94979732"
        ],
        [
            "Introduction to Data Visualization",
            "Francesca Albrezzi",
            "video workshop",
            "<p>Workshop slides: http://bit.ly/DataVizSlidesF2020\n<p> TOC:\n<ul>\n<li> 0:03:56 Overview on Data Types and Visualization Methods\n<li> 0:31:33 Open Refine Reconciliation Using the Getty Vocabularies\n<li> 1:08:33 Google Pivot Tables\n<li> 1:17:30 Tableau\n</ul>",
            "intro data visualization.png",
            "https://www.youtube.com/watch?v=-opw-P4dYEg",
            "data visualization, Tableau Software, Inc., pivot table, OpenRefine",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-opw-P4dYEg?si=fPpwIiz6YA9ZmGOH\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "data visualization, Tableau Software, Inc., pivot table, OpenRefine",
            "http://www.wikidata.org/entity/Q6504956; http://www.wikidata.org/entity/Q7673435; http://www.wikidata.org/entity/Q613906; http://www.wikidata.org/entity/Q5583871"
        ],
        [
            "Beginning Programming for Creatives",
            "Francesca Albrezzi",
            "video workshop",
            "<p>The digital realm is becoming an increasingly popular space for creatives. By creatives, I don't just mean artists, but anyone who wants to harness their creativity, skill, and imagination to produce something with a personal touch and flair. From interactivity to detailed design, browser windows can connect with viewers in powerful ways. So how do you start making the computer screen your canvas? It starts with programming. Programming is writing instructions that computers can understand and execute. For more than fifty years, computer programmers have been writing code and there are now more than 2,500 documented programming languages. So which language do you start with? What are the differences?\n<p>In this workshop, you will receive an introduction to the basics of programming. Specifically, we will cover:\n<ul>\n<li> What is programming \n<li> What is processing \n<li> The difference between Low-Level vs High Level programming languages \n<li> The difference between Client Side vs. Service Side Programs -p5.js, a free and open-source JavaScript library for creative coding, with a focus on making coding accessible and inclusive for artists, designers, educators, and beginners \n<li> How to use the p5.js web editor \n<li> How to create shapes and draw using p5.js\n</ul>\n<p>All levels welcome and no experience required. If you have any further questions regarding the workshop, please contact the instructor, Dr. Francesca Albrezzi.",
            "programming for creatives.png",
            "https://www.youtube.com/watch?v=6IhZICfe5A0",
            "computer programming, P5.js, JavaScript, HTML editor, creatives",
            "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6IhZICfe5A0?si=5lAuiK90cztS3Z0o\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>",
            "computer programming, P5.js, JavaScript, HTML editor",
            "http://www.wikidata.org/entity/Q80006; http://www.wikidata.org/entity/Q59555213; http://www.wikidata.org/entity/Q2005; http://www.wikidata.org/entity/Q726761"
        ],
        [
            "Polycam Photogrammetry Tutorial",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/presentation/d/11EPGcB3bT5RKkN5D8E-zC4-e3jU0f0P9K9CgQFQog7k/edit\">tutorial</a> will cover Polycam, an app that allows you to create photogrammetry captures and create 3D models all from your mobile device. In this method, you move around an object taking a lot of pictures. Then you feed those images into a software program that uses math to stitch them together, like a three-dimensional puzzle. The result is a 3D model that comes with a photorealistic texture layer.\n",
            "polycam.png",
            "",
            "photogrammetry, 3D modeling, polycam",
            "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRezMgbjoNtkCFOFkqy9Bz2AmbWcM2CLJnCyGV_zNYT-GCtKvNJyNpdsE2cXPhcKyx-f97o8TMRLef4/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "photogrammetry, 3D modeling",
            "http://www.wikidata.org/entity/Q190149; http://www.wikidata.org/entity/Q568742"
        ],
        [
            "SparkAR",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/presentation/d/1NPvUf2BX3yiFLWkhZhumG5lAnxhwfC1vDSUC4g0k0mg/edit\">tutorial</a> covers SparkAR, an application that allows you to create augmented reality experiences for Facebook, Facebook Messenger, and Instagram. The desktop application comes with built in resources, like asset libraries, to help get novices get started quickly, and powerful features to even let more advanced creatives use Javascript and VFX for advanced customization and artistic control. This tutorial will orient you to the interface, show you how to play around a bit, demonstrate a basic workflow, and point you to more resources.",
            "sparkar.png",
            "",
            "SparkAR, augmented reality",
            "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRcY_w7APgePIuoxYpqeA0jhIIdj2i1jMkBlt1d5dj9LEQ_45_NF4_wt16R6zVz5K5IZ6RoYd7QS0Db/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "augmented reality",
            "http://www.wikidata.org/entity/Q254183"
        ],
        [
            "Getting Started with TinkerCad",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/presentation/d/1Bzm2JUA5Tu6rJlw-_JAVkD_CazEwJ7o-WY6c1eB96Vk/edit?usp=sharing\">tutorial</a> will give you an introduction to Tinkercad, a free, easy-to-use app for 3D design, electronics, and coding. You will learn the basic interface of the app, become more familiar with 3D modeling itself, and build your own 3D model.  Other resources such as adding surface textures and importing your model into other platforms are also detailed.",
            "tinkercad.png",
            "",
            "Tinkercad, 3D modeling",
            "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vTXl_w0YNULHbPG6BnhG6u-IkCGRc8_sChphKLa3jgh0_aMgX7Lvl1n6P9NjQLVKP25KBZ9_dh1cFK6/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "Tinkercad, 3D modeling",
            "http://www.wikidata.org/entity/Q73124059; http://www.wikidata.org/entity/Q568742"
        ],
        [
            "Podcasting: How to Get Started",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/document/d/1E_23bj0RPBmyAci1W2s_FDfyCOI5hjPLiill5kWZEOA/edit?usp=sharing\">tutorial</a> will cover everything that you need to start your own podcast. Topics in this tutorial are how to organize your podcast, what roles are needed, common softwares used, editing your audio and getting the best sound quality, steps to take before, during, and after the podcast, and creative style tips to make your podcast your own.",
            "podcast.png",
            "",
            "podcast",
            "<iframe src=\"https://drive.google.com/file/d/1D7dSZfIfWn3UgU49cfR7uHuiphxak4C5/preview\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "podcast",
            "http://www.wikidata.org/entity/Q20899"
        ],
        [
            "Use 360\u00b0 Photos to Create a \u201cTiny Planet\u201d",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/presentation/d/1QNX_wNo7m8wZssnFKIvcqizpg-9dYpDluH6QK1DgVbY/edit#slide=id.g782bd57e60_1_6\">tutorial</a> will show you how to take 360\u00b0 photos or video and create a \u201ctiny planet\u201d effect using a free mobile app. In it you lean how to turn standard panoramas into miniature \u201cplanets\u201d that really have the \u201cwow\u201d factor.",
            "tiny planet.png",
            "",
            "360 photography, video recording, theta+",
            "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQL7H99L41ERlEVLSwESatNDGXEhxlu-3M_iPf73k64A-d-TUEP9-KnS_HCdAN6Xm1LDqNhc5b73PGl/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "360 photography, video recording",
            "http://www.wikidata.org/entity/Q27956049; http://www.wikidata.org/entity/Q34508"
        ],
        [
            "Artsteps: Building VR Exhibitions",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/document/d/1b22juru0kAkwS0M859d7oc846dWX6uUOFeBSqdudK8E/edit?usp=sharing\">tutorial</a> will show you how to use Artsteps, a browser-based platform that uses Unity tools to allow users to build and share their own virtual environments. We will walk you through making an Artsteps account, defining and designing your space, placing artifacts, planning a guided tour, and publishing your work.",
            "artsteps.png",
            "",
            "virtual reality, artsteps",
            "<iframe src=\"https://drive.google.com/file/d/1d1W5uQ0_RSl_S44tFNN2WJb4XMITH0VB/preview\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "virtual reality",
            "http://www.wikidata.org/entity/Q170519"
        ],
        [
            "VSim: An Interface for Academic 3D Models",
            "Francesca Albrezzi",
            "tutorial",
            "This <a href=\"https://docs.google.com/document/d/1ikwZQ8t3PXEJ_OX_kI4YFe0YSdSPlqdUSDUfQw1Ol4w/edit?usp=sharing\">tutorial</a> will provide you a walkthrough of VSim, an interface for exploring computer models of historic sites, environments, and objects. It is designed to support teacher-centered presentations and student-centered assignments, making it perfect for classroom use. We will walk you through downloading the application, navigating it, importing a model, building a narrative for the model, and saving and exporting your work.",
            "vsim.png",
            "",
            "3D modeling, VSim",
            "<iframe src=\"https://drive.google.com/file/d/1Qj75UZI5M6itjmo4LWT06_KhvtNPrZcx/preview\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>",
            "3D modeling",
            "http://www.wikidata.org/entity/Q568742"
        ]
    ],
    "consult": [
        [
            "Lisa Snyder, PhD",
            "Director, Campus Research Initiatives",
            "staff",
            "Lisa is OARC\u2019s Director of the Computational Research Technology Group and Research Initiatives. Through these roles, Lisa is responsible for providing institutional leadership and support to coordinate and build campus capacity for innovative research and pedagogy through institutional coordination, alignment, and development of IT system requirements related to research computing and research data management. She also works to facilitate broad-based, campus-wide collaboration and coordination with all academic departments, administrative and academic support units, campus computing facilities, and media/communications offices to align research and research computing with institutional direction.",
            "lisa.jpg",
            "https://oarc.ucla.edu/about/our-team/lisa-snyder",
            "visualization, 3D modeling, leadership, pedagogy, history of architecture",
            "visualization, 3D modeling, leadership, pedagogy, history of architecture",
            "http://www.wikidata.org/entity/Q451553; http://www.wikidata.org/entity/Q568742; http://www.wikidata.org/entity/Q484275; http://www.wikidata.org/entity/Q7922; http://www.wikidata.org/entity/Q8180985"
        ],
        [
            "Francesca Albrezzi, PhD",
            "Extended Reality (XR) Technologies in Research and Education",
            "staff",
            "Francesca Albrezzi, PhD, works at the intersection of humanities research and technology. As a digital research consultant at the Office of Advanced Research Computing, she assists faculty with their digital research needs, from ideation to execution and publication. Her dissertation \"Virtual Actualities: Technology, Museums, and Immersion\" (2019) interrogates modes of publishing, display, and information capture in museums and archives that illustrate a break from \u201ctraditional\u201d models, and argues that digital modalities provide a distinctly different paradigm for epistemologies of art and culture that offer greater contextualized understandings.",
            "francesca.jpg",
            "https://oarc.ucla.edu/about/our-team/francesca-albrezzi",
            "digital research, virtual reality, augmented reality, 360 photography, art history, digital humanities, 360 video",
            "digital research, virtual reality, augmented reality, 360 photography, art history, digital humanities",
            "http://www.wikidata.org/entity/Q99231518; http://www.wikidata.org/entity/Q170519; http://www.wikidata.org/entity/Q254183; http://www.wikidata.org/entity/Q27956049; http://www.wikidata.org/entity/Q50637; http://www.wikidata.org/entity/Q1026962"
        ],
        [
            "Ryan Horne, PhD",
            "Digital humanities research",
            "staff",
            "Ryan Horne, PhD, focuses on planning, managing, and preserving interdisciplinary digital humanities research projects. As a Research Consultant in the GIS, Visualization, XR & Modeling team at the Office of Advanced Research Computing, Ryan collaborates with campus researchers at all levels of their digital scholarship, including conceptualization, implementation, and publication.",
            "ryan.jpg",
            "https://oarc.ucla.edu/about/our-team/ryan-horne",
            "digital research, digital humanities, geographic information system, network analysis, Python, data visualization, study of history, linked open data, data modeling",
            "digital research, digital humanities, geographic information system, network analysis, Python, data visualization, study of history, linked open data, data modeling",
            "http://www.wikidata.org/entity/Q99231518; http://www.wikidata.org/entity/Q1026962; http://www.wikidata.org/entity/Q483130; http://www.wikidata.org/entity/Q4417999; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q6504956; http://www.wikidata.org/entity/Q1066186; http://www.wikidata.org/entity/Q18692990; http://www.wikidata.org/entity/Q367664"
        ],
        [
            "Yoh Kawano, PhD",
            "Sandbox Emeritus",
            "staff",
            "Yoh Kawano has lived across the globe, in five different countries, before joining the Faculty of Engineering at Reitaku University in 2022 as a Professor. Prior to Reitaku, he taught at UCLA\u2019s Urban Planning Department, teaching courses in GIS and Data Science. He also worked at UCLA\u2019s Office of Advanced Research Computing, where he served as a Lead Computational Scientist. He has supervised projects in urban planning, emergency preparedness, disaster relief, volunteerism, archaeology, social justice, and the digital humanities. ",
            "yoh.jpg",
            "https://www.reitaku-u.ac.jp/about/teachers/engineering/1777190/",
            "data science, geographic information system, ethnography, digital humanities, Python, data visualization",
            "data science, geographic information system, ethnography, digital humanities, Python, data visualization",
            "http://www.wikidata.org/entity/Q2374463; http://www.wikidata.org/entity/Q483130; http://www.wikidata.org/entity/Q132151; http://www.wikidata.org/entity/Q1026962; http://www.wikidata.org/entity/Q28865; http://www.wikidata.org/entity/Q6504956"
        ],
        [
            "Jessica",
            "DH Intern",
            "intern",
            "",
            "jessica.jpg",
            "",
            "internship"
        ],
        [
            "Angela",
            "DH Intern",
            "intern",
            "",
            "angela.jpg",
            "",
            "internship"
        ]
    ],
    "musings": [
        [
            "Adding URI Lookup from Wikidata",
            "Ryan Horne",
            "07/12/2024",
            "<p><a href=\"https://docs.google.com/spreadsheets\">Google Sheets</a> are a popular way to create and collaboratively edit tabular data in spreadsheets. Much like <a href=\"https://www.microsoft.com/en-us/microsoft-365/p/excel/cfq7ttc0pbmf\">Excel</a> and <a href=\"https://www.libreoffice.org/\">LibreOffice</a> you can use this program to sort, analyze and visualize data with the powerful tools and formulas which come standard in most spreadsheet applications.</p>\n<p>Sometimes you may desire more functionality than what a typical spreadsheet provides. In these cases <a href=\"https://developers.google.com/apps-script\">Google Apps Script</a> provides you with a javascript like environment for automatic tasks and enhancing functionality. Simply go to <em>Extenstions / App script</em> to open a new scripting environment.</p>\n<p>For this example, I wanted to make the process of reconciling the tags we use on the website with Wikidata IDs easier to enable more use of <a href=\"https://www.w3.org/egov/wiki/Linked_Open_Data\">Linked Open Data</a> resources. The script has two main objectives:</p>\n<ul>\n<li><p>Provide a search function within the sheet interface that uses the API of Wikidata to return results</p></li>\n<li><p>Automatically fill in a title and uri from Wikidata in two cells, and place in a delimiter between each entry.</p></li>\n</ul>\n<p>This was accomplished with the following code:</p>\n<p>\n\n<pre><code class=\"language-javascript\">\nfunction onOpen() { \n\tSpreadsheetApp.getUi() \n\t\t.createMenu(\u2018Wikidata Search\u2019) \n\t\t.addItem(\u2018Open Sidebar\u2019, \u2018openSidebar\u2019) \n\t\t.addToUi(); \n}\n\nfunction openSidebar() { \n\tvar html = HtmlService.createHtmlOutputFromFile(\u2018Sidebar\u2019) \n\t\t.setTitle(\u2018Wikidata Fuzzy Search\u2019); \n\tSpreadsheetApp.getUi().showSidebar(html); \n}\n\nfunction fetchWikidata(term) { \n\ttry { \n\t\tvar apiUrl = \u2018https://www.wikidata.org/w/api.php?action=wbsearchentities&search=\u2019 + encodeURIComponent(term) + \u2018&language=en&format=json\u2019; \n\t\tvar response = UrlFetchApp.fetch(apiUrl); \n\t\tvar json = JSON.parse(response.getContentText()); \n\t\tvar results = []; \n\t\tif (json.search && json.search.length > 0) { \n\t\t\tfor (var i = 0; i < json.search.length; i++) { \n\t\t\t\tresults.push([json.search[i].label, json.search[i].description || \u2018No description available\u2019, json.search[i].concepturi]); \n\t\t\t} else { \n\t\t\t\tresults.push([\u201cNot Found\u201d, \u201cNo description available\u201d, \u201cNot Found\u201d]); \n\t\t\t} \n\t\treturn results; \n\t} catch (e) { \n\t\tLogger.log(e.toString()); \n\t\tthrow new Error(\u2018Failed to fetch Wikidata. Please ensure you have granted the necessary permissions.\u2019); \n\t} \n}\n\nfunction insertResult(title, uri) { \n\ttry { \n\t\tvar sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet(); \n\t\tvar range = sheet.getActiveCell(); \n\t\tvar currentTitle = range.getValue(); \n\t\tvar currentUri = range.offset(0, 1).getValue();\n\n\t\tif (currentTitle) {\n\t\t\ttitle = currentTitle + '; ' + title;\n\t\t}\n\t\tcd(currentUri) {\n\t\t\turi = currentUri + '; ' + uri;\n\t\t}\n\t\t\n\t\trange.setValue(title);\n\t\trange.offset(0, 1).setValue(uri);\n\t} catch (e) {\n\t\tLogger.log(e.toString());\n\t\tthrow new Error('Failed to insert result into the spreadsheet. Please ensure you have granted the necessary permissions.');\n\t}\n}\n</code></pre>\n\n<p>We also need to style a sidebar, which we do by creating <em>Sidebar.html</em> It includes the following javascript functions:</p>\n\n<pre><code class=\"language-javascript\">\nfunction searchWikidata() {\n\tvar term = document.getElementById('searchTerm').value;\n\tgoogle.script.run\n\t.withSuccessHandler(displayResults)\n\t.withFailureHandler(showError)\n\t.fetchWikidata(term);\n}\n\nfunction displayResults(results) {\n\tvar ul = document.getElementById('results');\n\tul.innerHTML = '';\n\tresults.forEach(function(result) {\n\tvar li = document.createElement('li');\n\tvar titleSpan = document.createElement('span');\n\tvar descriptionSpan = document.createElement('span');\n\tvar link = document.createElement('a');\n\n\ttitleSpan.textContent = result[0];\n\ttitleSpan.className = 'title';\n\tdescriptionSpan.textContent = \" - \" + result[1];\n\tdescriptionSpan.className = 'description';\n\tlink.href = result[2];\n\tlink.target = '_blank';\n\tlink.textContent = result[2];\n\n\ttitleSpan.ondblclick = function() {\n\t\tgoogle.script.run.insertResult(result[0], result[2]);\n\t};\n\n\tli.appendChild(titleSpan);\n\tli.appendChild(descriptionSpan);\n\tli.appendChild(document.createTextNode(' - '));\n\tli.appendChild(link);\n\tul.appendChild(li);\n\t});\n}\n\nfunction showError(error) {\n\tvar errorMessageDiv = document.getElementById('error-message');\n\terrorMessageDiv.textContent = 'Error: ' + error.message;\n}\n</pre></code>\n\n<p>After this there should now be a new Wikidata menu item. Click on that, open the sidebar, and happy matching!</p>\n\n<p>One caveat to these scripts is that authorization has to be granted to each Google user account which you want to access the script. You may encounter a <em>permission needed</em> error when running the script; if this is the case, ensure that you are completely logged out of google on <strong>ALL</strong> your accounts, then log back in with the authorized account and the script should work as intended.</p>\n\n",
            "wikidata.png",
            "",
            "test"
        ],
        [
            "Book Review: Unmasking AI",
            "Francesca Albrezzi",
            "8/28/2024",
            "<p><a href=\"https://docs.google.com/spreadsheets\">Google Sheets</a> are a popular way to create and collaboratively edit tabular data in spreadsheets. Much like <a href=\"https://www.microsoft.com/en-us/microsoft-365/p/excel/cfq7ttc0pbmf\">Excel</a> and <a href=\"https://www.libreoffice.org/\">LibreOffice</a> you can use this program to sort, analyze and visualize data with the powerful tools and formulas which come standard in most spreadsheet applications.</p>\n<p>Sometimes you may desire more functionality than what a typical spreadsheet provides. In these cases <a href=\"https://developers.google.com/apps-script\">Google Apps Script</a> provides you with a javascript like environment for automatic tasks and enhancing functionality. Simply go to <em>Extenstions / App script</em> to open a new scripting environment.</p>\n<p>For this example, I wanted to make the process of reconciling the tags we use on the website with Wikidata IDs easier to enable more use of <a href=\"https://www.w3.org/egov/wiki/Linked_Open_Data\">Linked Open Data</a> resources. The script has two main objectives:</p>\n<ul>\n<li><p>Provide a search function within the sheet interface that uses the API of Wikidata to return results</p></li>\n<li><p>Automatically fill in a title and uri from Wikidata in two cells, and place in a delimiter between each entry.</p></li>\n</ul>\n<p>This was accomplished with the following code:</p>\n<p>\n\n<pre><code class=\"language-javascript\">\nfunction onOpen() { \n\tSpreadsheetApp.getUi() \n\t\t.createMenu(\u2018Wikidata Search\u2019) \n\t\t.addItem(\u2018Open Sidebar\u2019, \u2018openSidebar\u2019) \n\t\t.addToUi(); \n}\n\nfunction openSidebar() { \n\tvar html = HtmlService.createHtmlOutputFromFile(\u2018Sidebar\u2019) \n\t\t.setTitle(\u2018Wikidata Fuzzy Search\u2019); \n\tSpreadsheetApp.getUi().showSidebar(html); \n}\n\nfunction fetchWikidata(term) { \n\ttry { \n\t\tvar apiUrl = \u2018https://www.wikidata.org/w/api.php?action=wbsearchentities&search=\u2019 + encodeURIComponent(term) + \u2018&language=en&format=json\u2019; \n\t\tvar response = UrlFetchApp.fetch(apiUrl); \n\t\tvar json = JSON.parse(response.getContentText()); \n\t\tvar results = []; \n\t\tif (json.search && json.search.length > 0) { \n\t\t\tfor (var i = 0; i < json.search.length; i++) { \n\t\t\t\tresults.push([json.search[i].label, json.search[i].description || \u2018No description available\u2019, json.search[i].concepturi]); \n\t\t\t} else { \n\t\t\t\tresults.push([\u201cNot Found\u201d, \u201cNo description available\u201d, \u201cNot Found\u201d]); \n\t\t\t} \n\t\treturn results; \n\t} catch (e) { \n\t\tLogger.log(e.toString()); \n\t\tthrow new Error(\u2018Failed to fetch Wikidata. Please ensure you have granted the necessary permissions.\u2019); \n\t} \n}\n\nfunction insertResult(title, uri) { \n\ttry { \n\t\tvar sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet(); \n\t\tvar range = sheet.getActiveCell(); \n\t\tvar currentTitle = range.getValue(); \n\t\tvar currentUri = range.offset(0, 1).getValue();\n\n\t\tif (currentTitle) {\n\t\t\ttitle = currentTitle + '; ' + title;\n\t\t}\n\t\tcd(currentUri) {\n\t\t\turi = currentUri + '; ' + uri;\n\t\t}\n\t\t\n\t\trange.setValue(title);\n\t\trange.offset(0, 1).setValue(uri);\n\t} catch (e) {\n\t\tLogger.log(e.toString());\n\t\tthrow new Error('Failed to insert result into the spreadsheet. Please ensure you have granted the necessary permissions.');\n\t}\n}\n</code></pre>\n\n<p>We also need to style a sidebar, which we do by creating <em>Sidebar.html</em></p>\n\n<pre><code class=\"language-markup\">\n<style>\n\t.title {\n\t  font-weight: bold;\n\t  font-size: 1.1em; /* Adjust the size as needed */\n\t  cursor: pointer;\n\t}\n\t.description {\n\t  font-size: 1em; /* Slightly smaller font for the description */\n\t}\n\tli {\n\t  margin-bottom: 10px;\n\t}\n</style>\n\n\n<input type=\"text\" id=\"searchTerm\" placeholder=\"Enter search term\">\n<button onclick=\"searchWikidata()\">Search</button>\n<ul id=\"results\"></ul>\n<div id=\"error-message\" style=\"color: red;\"></div>\n\n<script>\n\tfunction searchWikidata() {\n\t  var term = document.getElementById('searchTerm').value;\n\t  google.script.run\n\t\t.withSuccessHandler(displayResults)\n\t\t.withFailureHandler(showError)\n\t\t.fetchWikidata(term);\n\t}\n\n\tfunction displayResults(results) {\n\t  var ul = document.getElementById('results');\n\t  ul.innerHTML = '';\n\t  results.forEach(function(result) {\n\t\tvar li = document.createElement('li');\n\t\tvar titleSpan = document.createElement('span');\n\t\tvar descriptionSpan = document.createElement('span');\n\t\tvar link = document.createElement('a');\n\n\t\ttitleSpan.textContent = result[0];\n\t\ttitleSpan.className = 'title';\n\t\tdescriptionSpan.textContent = \" - \" + result[1];\n\t\tdescriptionSpan.className = 'description';\n\t\tlink.href = result[2];\n\t\tlink.target = '_blank';\n\t\tlink.textContent = result[2];\n\n\t\ttitleSpan.ondblclick = function() {\n\t\t  google.script.run.insertResult(result[0], result[2]);\n\t\t};\n\n\t\tli.appendChild(titleSpan);\n\t\tli.appendChild(descriptionSpan);\n\t\tli.appendChild(document.createTextNode(' - '));\n\t\tli.appendChild(link);\n\t\tul.appendChild(li);\n\t  });\n\t}\n\n\tfunction showError(error) {\n\t  var errorMessageDiv = document.getElementById('error-message');\n\t  errorMessageDiv.textContent = 'Error: ' + error.message;\n\t}\n</script>\n\n\n</pre></code>\n\n<p>After this there should now be a new Wikidata menu item. Click on that, open the sidebar, and happy matching!</p>\n\n<p>One caveat to these scripts is that authorization has to be granted to each Google user account which you want to access the script. You may encounter a <em>permission needed</em> error when running the script; if this is the case, ensure that you are completely logged out of google on <strong>ALL</strong> your accounts, then log back in with the authorized account and the script should work as intended.</p>\n\n",
            "unmaskingAI.png",
            "",
            "book review, AI, ethics"
        ],
        [
            "Book Review: Unmasking AI",
            "Francesca Albrezzi",
            "8/28/2024",
            "</style></head><body class=\"c11 doc-content\"><h2 class=\"c7 c13\" id=\"h.kvs0g9vn1rcq\"><span class=\"c0 c12\">Book Review: Unmasking AI by Joy Buolamwini</span></h2><p class=\"c7\"><span class=\"c3 c10\">Note: This review was co-written with AI.</span></p><p class=\"c7\"><span>In the rapidly evolving landscape of technology, Joy Buolamwini&rsquo;s </span><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;stands as a crucial and timely work. This book delves deep into the ethical challenges posed by artificial intelligence, especially in terms of bias and discrimination. Buolamwini, a renowned computer scientist and activist, combines her personal experiences with rigorous research to expose the often-hidden biases embedded in AI systems.</span></p><h3 class=\"c8\" id=\"h.rb00x0e5ob5t\"><span class=\"c9 c0\">A Journey Through AI&#39;s Ethical Landscape</span></h3><p class=\"c7\"><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;is more than just a critique of technology; it&rsquo;s a call to action. Buolamwini draws on her groundbreaking work with the Algorithmic Justice League to highlight how AI systems can perpetuate and even amplify societal inequalities. She examines the historical context of these biases, linking them to broader issues of race, gender, and power.</span></p><p class=\"c7\"><span class=\"c2\">For students and educators in Digital Humanities, this book is an essential resource. It provides a framework for understanding the intersection of technology, ethics, and society&mdash;a key concern for anyone studying how digital tools impact the human experience.</span></p><h3 class=\"c8\" id=\"h.avt7omradftb\"><span class=\"c9 c0\">Key Themes, Insights, and Projects</span></h3><p class=\"c7\"><span class=\"c2\">One of the book&#39;s most powerful messages is the idea that technology is not neutral. Buolamwini reveals how AI systems, often seen as objective and impartial, can reflect and reinforce the prejudices of their creators. This is particularly evident in facial recognition technologies, which have been shown to misidentify people of color at alarmingly high rates. </span></p><p class=\"c7\"><span class=\"c2\">Buolamwini&rsquo;s narrative is also deeply personal. She shares her journey from being a student encountering biased AI in her research to becoming a leading voice advocating for fairness in technology. This personal touch makes the book not only informative but also deeply engaging.</span></p><p class=\"c7\"><span>As an artist and a programmer, Buolamwini has produced videos that emphasize key aspects of her work. One of the first she mentions in her books is </span><span class=\"c6 c3\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.youtube.com/watch?v%3D162VzSzzoPs&amp;sa=D&amp;source=editors&amp;ust=1724889491575760&amp;usg=AOvVaw0Oeh9CcfY-V_wA9JicBSwT\">The Coded Gaze: Unmasking Algorithmic Bias</a></span><span>, and she also featured the related work within a related </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?utm_campaign%3Dtedspread%26utm_medium%3Dreferral%26utm_source%3Dtedcomshare&amp;sa=D&amp;source=editors&amp;ust=1724889491576275&amp;usg=AOvVaw17mU5MhIzUjPaymv56d40L\">TED Talk</a></span><span class=\"c2\">. </span></p><p class=\"c7\"><span>For her thesis, she began what became known as the Gender Shades project. An </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://youtu.be/TWWsW1w-BVo&amp;sa=D&amp;source=editors&amp;ust=1724889491576697&amp;usg=AOvVaw0KqG_gm6XrWCsvI7NuppRf\">overview video</a></span><span>&nbsp;of the project is available, as well as a </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=http://gendershades.org/overview.html&amp;sa=D&amp;source=editors&amp;ust=1724889491577041&amp;usg=AOvVaw0GpbBiIVfc6Ff8w5Wa6JXv\">scrolling interactive digital walkthrough</a></span><span>. Buolamwini reflected on results in the work by creating a spoken word poem, which she went on to pair with powerful images in the video: </span><span class=\"c6 c3\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.youtube.com/watch?v%3DQxuyfWoVV98&amp;sa=D&amp;source=editors&amp;ust=1724889491577384&amp;usg=AOvVaw1CABHLj4PDLBDbjdIuFnzn\">AI, Ain&#39;t I A Woman?</a></span></p><p class=\"c7\"><span>Her work inspired her to found the </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.ajl.org/&amp;sa=D&amp;source=editors&amp;ust=1724889491577743&amp;usg=AOvVaw3oVZhRkwM4Ngv4NAdIopqU\">Algorithmic Justice League</a></span><span>&nbsp;and you can find more about her and her research on her personal website: </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://poetofcode.com/?home&amp;sa=D&amp;source=editors&amp;ust=1724889491577986&amp;usg=AOvVaw3fGhi1Pwqt36dd8Lxf8o6i\">https://poetofcode.com/?home</a></span><span>.</span></p><h3 class=\"c8\" id=\"h.nw2qjw7jk35s\"><span class=\"c9 c0\">Why It Matters for Digital Humanities</span></h3><p class=\"c7\"><span>In Digital Humanities, where the focus is on using digital tools to study human culture, understanding the biases in these tools is critical. </span><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;provides the theoretical and practical knowledge necessary to critically engage with digital technologies. Whether you&rsquo;re working on text analysis, digital mapping, or any other DH project, this book encourages you to think about the ethical implications of the tools you&rsquo;re using.</span></p><h3 class=\"c8\" id=\"h.d6ww7ja75i0y\"><span class=\"c0 c9\">Classroom Applications</span></h3><p class=\"c7\"><span>For educators, </span><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;can be a valuable addition to your syllabus. Here are a few ways to incorporate it into your Digital Humanities curriculum:</span></p><ol class=\"c5 lst-kix_42197dhzyyid-0 start\" start=\"1\"><li class=\"c4 li-bullet-0\"><span class=\"c0\">Ethics Modules:</span><span class=\"c2\">&nbsp;Use the book to introduce students to ethical issues in technology. Discuss the case studies Buolamwini presents and have students explore how these issues might arise in their own work.</span></li><li class=\"c4 li-bullet-0\"><span class=\"c0\">Critical Discussions:</span><span class=\"c2\">&nbsp;Pair the book with other readings on digital ethics and have students debate the responsibilities of technologists and digital humanists in creating and using AI.</span></li><li class=\"c4 li-bullet-0\"><span class=\"c0\">Project-Based Learning:</span><span>&nbsp;Encourage students to investigate a specific AI tool or platform, using the concepts from </span><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;to analyze its potential biases and societal impacts.</span></li></ol><h3 class=\"c8\" id=\"h.xat0cg3ek0di\"><span class=\"c9 c0\">Related Resources</span></h3><p class=\"c7\"><span>To further enrich your understanding and teaching of the topics covered in </span><span class=\"c3\">Unmasking AI</span><span class=\"c2\">, here are some related resources:</span></p><ul class=\"c5 lst-kix_9e9yc5o91gbz-0 start\"><li class=\"c4 li-bullet-0\"><span class=\"c0\">Algorithmic Justice League</span><span>:</span><span><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.ajlunited.org/&amp;sa=D&amp;source=editors&amp;ust=1724889491579930&amp;usg=AOvVaw1y4AJylo4tAPTvR7N9nsB6\">&nbsp;</a></span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://www.ajlunited.org/&amp;sa=D&amp;source=editors&amp;ust=1724889491580141&amp;usg=AOvVaw2ojYMx9w4YLfqv644LSB6F\">Visit the website</a></span><span class=\"c2\">&nbsp;for more information on Buolamwini&rsquo;s work and access to resources, including documentaries and tools for assessing AI bias.</span></li><li class=\"c4 li-bullet-0\"><span class=\"c0\">Data Feminism by Catherine D&rsquo;Ignazio and Lauren F. Klein</span><span>: This book explores similar themes from a feminist perspective, making it a great companion to </span><span class=\"c3\">Unmasking AI</span><span>. </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://data-feminism.mitpress.mit.edu/&amp;sa=D&amp;source=editors&amp;ust=1724889491580723&amp;usg=AOvVaw1fLlImw6YXus_JBJf9RIji\">Learn more.</a></span></li><li class=\"c4 li-bullet-0\"><span class=\"c0\">AI Now Institute</span><span>: An interdisciplinary research institute dedicated to understanding the social implications of AI. </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://ainowinstitute.org/&amp;sa=D&amp;source=editors&amp;ust=1724889491581306&amp;usg=AOvVaw2tBCt7AI5nZgPCra4eaqGe\">Their reports and publications</a></span><span class=\"c2\">&nbsp;provide in-depth analyses of AI&rsquo;s impact on society.</span></li><li class=\"c4 li-bullet-0\"><span class=\"c0\">Ethics of AI</span><span>: A comprehensive set of resources are available from the Future of Life Institute that covers a wide range of ethical issues related to AI. </span><span class=\"c6\"><a class=\"c1\" href=\"https://www.google.com/url?q=https://futureoflife.org/cause-area/artificial-intelligence/&amp;sa=D&amp;source=editors&amp;ust=1724889491581898&amp;usg=AOvVaw0R4eEN6K4GCNRsyOH_DQRf\">Explore now.</a></span></li></ul><h3 class=\"c8\" id=\"h.56b41j61au0v\"><span class=\"c9 c0\">Conclusion</span></h3><p class=\"c7\"><span class=\"c3\">Unmasking AI</span><span class=\"c2\">&nbsp;is not just a book; it&rsquo;s a movement. For students and scholars in Digital Humanities, it offers a critical lens through which to view the digital tools that are increasingly shaping our world. By engaging with Buolamwini&rsquo;s insights, we can work toward a future where technology serves all of humanity&mdash;fairly and justly.</span></p></body></html>",
            "unmaskingAI.png",
            "",
            "book review, AI, ethics"
        ]
    ]
}